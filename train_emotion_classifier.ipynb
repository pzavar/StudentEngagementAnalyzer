{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Emotion Classification Model\n",
    "\n",
    "This notebook will guide you through training the emotion classification model for student engagement analysis.\n",
    "\n",
    "## Step 1: Setup Environment\n",
    "\n",
    "First, make sure you have all required packages installed. Run this cell to install them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install tensorflow pandas numpy matplotlib seaborn scikit-learn opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Required Libraries\n",
    "\n",
    "Now let's import all the libraries we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load and Prepare Data\n",
    "\n",
    "Download the [Emotion Recognition Dataset](https://www.kaggle.com/datasets/sujaykapadnis/emotion-recognition-dataset/data) from Kaggle and extract it. \n",
    "\n",
    "The dataset should have a structure like this:\n",
    "```\n",
    "dataset/\n",
    "    train/\n",
    "        angry/\n",
    "        happy/\n",
    "        neutral/\n",
    "        sad/\n",
    "        surprise/\n",
    "```\n",
    "\n",
    "Now let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_dataframe(data_dir):\n",
    "    \"\"\"Create DataFrame with image paths and labels\"\"\"\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    emotions = ['angry', 'happy', 'neutral', 'sad', 'surprise']\n",
    "    \n",
    "    for i, emotion in enumerate(emotions):\n",
    "        path = f\"{data_dir}/{emotion}\"\n",
    "        for img in os.listdir(path):\n",
    "            if img.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                image_paths.append(os.path.join(path, img))\n",
    "                labels.append(i)\n",
    "    \n",
    "    return pd.DataFrame({'image_path': image_paths, 'emotion': labels})\n",
    "\n",
    "# Create DataFrame\n",
    "data_dir = \"path/to/dataset/train\"  # Replace with your dataset path\n",
    "df = create_dataframe(data_dir)\n",
    "\n",
    "# Display sample of the data\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create and Compile Model\n",
    "\n",
    "We'll use MobileNetV2 as our base model and add custom layers for emotion classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def create_model(input_shape=(224, 224, 3)):\n",
    "    \"\"\"Create and compile the model\"\"\"\n",
    "    # Base model - MobileNetV2\n",
    "    base_model = MobileNetV2(\n",
    "        weights='imagenet',  # Use pre-trained weights\n",
    "        include_top=False,   # Don't include the classification layers\n",
    "        input_shape=input_shape\n",
    "    )\n",
    "    \n",
    "    # Freeze base model layers\n",
    "    base_model.trainable = False\n",
    "    \n",
    "    # Add classification head\n",
    "    x = GlobalAveragePooling2D()(base_model.output)\n",
    "    x = Dropout(0.5)(x)  # Prevent overfitting\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    outputs = Dense(5, activation='softmax')(x)  # 5 emotion classes\n",
    "    \n",
    "    model = Model(inputs=base_model.input, outputs=outputs)\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Create model\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Prepare Data Generator\n",
    "\n",
    "We'll create a data generator to efficiently load and preprocess images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def preprocess_data(df, input_shape=(224, 224)):\n",
    "    \"\"\"Preprocess images and prepare labels\"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Load and preprocess image\n",
    "        img = tf.keras.preprocessing.image.load_img(\n",
    "            row['image_path'],\n",
    "            target_size=input_shape\n",
    "        )\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = tf.keras.applications.mobilenet_v2.preprocess_input(img_array)\n",
    "        X.append(img_array)\n",
    "        y.append(row['emotion'])\n",
    "        \n",
    "        # Print progress\n",
    "        if (idx + 1) % 1000 == 0:\n",
    "            print(f\"Processed {idx + 1} images\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = tf.keras.utils.to_categorical(y)\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Preprocess data\n",
    "print(\"Preprocessing training data...\")\n",
    "X, y = preprocess_data(df)\n",
    "\n",
    "# Split data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,  # Use 20% for validation\n",
    "    random_state=42,  # For reproducibility\n",
    "    stratify=y  # Maintain class distribution\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Train the Model\n",
    "\n",
    "Now let's train the model with callbacks for better training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Setup callbacks for better training\n",
    "callbacks = [\n",
    "    # Save the best model\n",
    "    ModelCheckpoint(\n",
    "        'best_model.h5',  # Save model to this file\n",
    "        monitor='val_accuracy',  # Watch validation accuracy\n",
    "        save_best_only=True,  # Only save if better than previous\n",
    "        mode='max',  # Higher accuracy is better\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Stop if not improving\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',  # Watch validation loss\n",
    "        patience=10,  # Wait 10 epochs for improvement\n",
    "        restore_best_weights=True,  # Use best weights when done\n",
    "        verbose=1\n",
    "    ),\n",
    "    # Reduce learning rate when stuck\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.1,  # Reduce LR by 90%\n",
    "        patience=5,  # Wait 5 epochs before reducing\n",
    "        min_lr=1e-6,  # Don't go below this LR\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,  # Maximum number of epochs\n",
    "    batch_size=32,  # Process 32 images at a time\n",
    "    callbacks=callbacks,\n",
    "    verbose=1  # Show progress\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Training Results\n",
    "\n",
    "Let's plot the training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def plot_training_results(history):\n",
    "    \"\"\"Plot training metrics\"\"\"\n",
    "    # Set style\n",
    "    sns.set_style(\"whitegrid\")\n",
    "    sns.set_palette(\"husl\")\n",
    "    \n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot results\n",
    "plot_training_results(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save the Model\n",
    "\n",
    "The model has already been saved as 'best_model.h5' through the ModelCheckpoint callback during training. You can now copy this file to your Student Engagement Analysis project directory.\n",
    "\n",
    "To verify the model was saved:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "if os.path.exists('best_model.h5'):\n",
    "    print(\"Model saved successfully!\")\n",
    "    print(f\"Model file size: {os.path.getsize('best_model.h5') / (1024*1024):.2f} MB\")\n",
    "else:\n",
    "    print(\"Error: Model file not found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}